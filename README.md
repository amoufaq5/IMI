# IMI - Intelligent Medical Intelligence

[![Python](https://img.shields.io/badge/python-3.11+-blue.svg)]()
[![PyTorch](https://img.shields.io/badge/PyTorch-2.2+-red.svg)]()
[![License](https://img.shields.io/badge/license-MIT-green.svg)]()

> **Fine-tuned Medical LLM with Reinforcement Learning for Enhanced Reasoning**

## ğŸ¯ Overview

IMI is a medical AI training pipeline that fine-tunes large language models on medical data with reinforcement learning for improved reasoning. The trained model powers three specialized applications:

- **ğŸ’Š Pharma App**: Drug discovery, clinical trials, regulatory affairs
- **ğŸ“š Student App**: Medical education, USMLE prep, clinical reasoning
- **ğŸ¥ General App**: Health information for general users

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      IMI Training Pipeline                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Data Ingestion    â”‚  Data Processing   â”‚  Training             â”‚
â”‚  - PubMed          â”‚  - QA Generation   â”‚  - SFT (Supervised)   â”‚
â”‚  - HuggingFace     â”‚  - Deduplication   â”‚  - DPO (Preference)   â”‚
â”‚  - Medical QA      â”‚  - Format Convert  â”‚  - ORPO (Combined)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                      Fine-tuned Medical LLM                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Pharma App        â”‚  Student App       â”‚  General Health App   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites
- Python 3.11+
- CUDA 11.8+ (for GPU training)
- 24GB+ VRAM (for QLoRA) or 80GB+ (for full fine-tune)

### Installation

```bash
# Clone repository
git clone https://github.com/your-org/imi.git
cd imi

# Create virtual environment
python -m venv .venv
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Install flash-attention (optional, for long sequences)
pip install flash-attn --no-build-isolation
```

### Full Training Pipeline

```bash
# Run complete pipeline: Ingest â†’ Process â†’ SFT â†’ DPO
python scripts/training/run_training.py --all

# Or run individual stages:
python scripts/training/run_training.py --ingest    # Data ingestion
python scripts/training/run_training.py --process   # Data processing
python scripts/training/run_training.py --sft       # Supervised fine-tuning
python scripts/training/run_training.py --dpo       # Preference optimization

# Alternative: ORPO (combined SFT + preference, more efficient)
python scripts/training/run_training.py --orpo
```

### Launch Applications

```bash
# Pharma App (port 7860)
python apps/pharma/app.py --model outputs/imi-medical/sft

# Student App (port 7861)
python apps/student/app.py --model outputs/imi-medical/sft

# General Health App (port 7862)
python apps/general/app.py --model outputs/imi-medical/sft
```

## ğŸ“ Project Structure

```
imi/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ data_ingestion/          # Data scrapers
â”‚   â”‚   â”œâ”€â”€ base_scraper.py      # Base scraper class
â”‚   â”‚   â”œâ”€â”€ scrape_pubmed.py     # PubMed literature
â”‚   â”‚   â”œâ”€â”€ scrape_medical_datasets.py  # HuggingFace datasets
â”‚   â”‚   â””â”€â”€ scrape_all.py        # Master ingestion script
â”‚   â”‚
â”‚   â””â”€â”€ training/                # Training pipeline
â”‚       â”œâ”€â”€ data_processor.py    # Convert to training format
â”‚       â”œâ”€â”€ sft_trainer.py       # Supervised fine-tuning
â”‚       â”œâ”€â”€ dpo_trainer.py       # Direct Preference Optimization
â”‚       â”œâ”€â”€ orpo_trainer.py      # Odds Ratio Preference Optimization
â”‚       â””â”€â”€ run_training.py      # Master training script
â”‚
â”œâ”€â”€ apps/                        # User applications
â”‚   â”œâ”€â”€ pharma/app.py           # Pharmaceutical research assistant
â”‚   â”œâ”€â”€ student/app.py          # Medical education assistant
â”‚   â””â”€â”€ general/app.py          # General health assistant
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # Scraped data
â”‚   â””â”€â”€ processed/              # Training-ready data
â”‚
â”œâ”€â”€ outputs/                    # Trained models
â”‚
â””â”€â”€ requirements.txt            # Dependencies
```

## ğŸ”§ Training Configuration

### SFT (Supervised Fine-Tuning)

```bash
python scripts/training/sft_trainer.py \
    --model mistralai/Mistral-7B-Instruct-v0.3 \
    --mode qlora \
    --max-seq-length 4096 \
    --batch-size 2 \
    --grad-accum 8 \
    --lr 2e-4 \
    --epochs 3 \
    --lora-r 64
```

### DPO (Direct Preference Optimization)

```bash
python scripts/training/dpo_trainer.py \
    --model outputs/imi-medical/sft \
    --beta 0.1 \
    --lr 5e-5 \
    --epochs 1
```

### ORPO (Combined SFT + Preference)

```bash
python scripts/training/orpo_trainer.py \
    --model mistralai/Mistral-7B-Instruct-v0.3 \
    --beta 0.1 \
    --lr 8e-6 \
    --epochs 3
```

## ğŸ“Š Training Modes

| Mode | VRAM Required | Speed | Quality |
|------|---------------|-------|---------|
| QLoRA (4-bit) | 24GB | Fast | Good |
| LoRA (16-bit) | 48GB | Medium | Better |
| Full Fine-tune | 80GB+ | Slow | Best |

## ğŸ§ª Data Sources

### Scraped Data
- **PubMed**: Medical literature and research articles
- **HuggingFace Datasets**:
  - PubMedQA: Research question answering
  - MedQA: USMLE-style questions
  - MedMCQA: Medical entrance exam questions
  - Medical Meadow: Curated medical QA
  - ChatDoctor: Doctor-patient conversations

### Training Formats
- **SFT**: Chat format with system prompts
- **DPO/ORPO**: Preference pairs (chosen/rejected responses)

## ğŸ”¬ Reinforcement Learning

IMI supports multiple RL approaches:

1. **DPO (Direct Preference Optimization)**
   - No reward model needed
   - Stable training
   - Good for preference alignment

2. **ORPO (Odds Ratio Preference Optimization)**
   - Combines SFT and preference learning
   - Single training stage
   - More memory efficient

3. **PPO (Proximal Policy Optimization)** *(coming soon)*
   - Classic RLHF approach
   - Requires reward model
   - Most flexible

## ğŸ“ˆ Experiment Tracking

```bash
# Enable Weights & Biases logging
python scripts/training/run_training.py --all --wandb

# View training metrics
wandb login
# Then check your W&B dashboard
```

## ğŸš€ Deployment

### Local Inference
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# Load model
tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.3")
model = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-Instruct-v0.3")
model = PeftModel.from_pretrained(model, "outputs/imi-medical/sft")

# Generate
messages = [{"role": "user", "content": "What are the symptoms of diabetes?"}]
prompt = tokenizer.apply_chat_template(messages, tokenize=False)
outputs = model.generate(tokenizer(prompt, return_tensors="pt").input_ids)
print(tokenizer.decode(outputs[0]))
```

### vLLM Serving (Production)
```bash
python -m vllm.entrypoints.openai.api_server \
    --model outputs/imi-medical/sft \
    --port 8000
```

## ğŸ“„ License

MIT License - See [LICENSE](LICENSE) for details.

---

**Built for better medical AI ğŸ¥**
